"""Initial schema check after adding IP

Revision ID: f5b76ed1b9bd
Revises:
Create Date: 2025-04-29 13:04:39.696706

"""

from typing import Sequence, Union

import sqlalchemy as sa

# Import func if needed for server defaults, though NOW() is standard SQL
# from sqlalchemy.sql import func
from sqlalchemy.dialects import postgresql

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "f5b76ed1b9bd"
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "reflection_logs",
        sa.Column(
            "id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            server_default=sa.text("gen_random_uuid()"),
        ),
        sa.Column("user_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column(
            "timestamp",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column("reflection_text", sa.Text(), nullable=False),
        sa.Column(
            "snapshot_ref", postgresql.JSONB(astext_type=sa.Text()), nullable=True
        ),
        sa.Column(
            "analysis_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True
        ),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["users.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_reflection_logs_id"), "reflection_logs", ["id"], unique=False
    )
    op.create_index(
        op.f("ix_reflection_logs_user_id"), "reflection_logs", ["user_id"], unique=False
    )
    op.create_table(
        "task_footprints",
        sa.Column(
            "id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            server_default=sa.text("gen_random_uuid()"),
        ),
        sa.Column("user_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("task_id", sa.String(), nullable=False),
        sa.Column("event_type", sa.String(), nullable=False),
        sa.Column(
            "timestamp",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "snapshot_ref", postgresql.JSONB(astext_type=sa.Text()), nullable=True
        ),
        sa.Column(
            "event_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True
        ),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["users.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_task_footprints_id"), "task_footprints", ["id"], unique=False
    )
    op.create_index(
        op.f("ix_task_footprints_task_id"), "task_footprints", ["task_id"], unique=False
    )
    op.create_index(
        op.f("ix_task_footprints_user_id"), "task_footprints", ["user_id"], unique=False
    )
    op.drop_index("ix_reflection_event_logs_id", table_name="reflection_event_logs")
    op.drop_index(
        "ix_reflection_event_logs_linked_hta_node_id",
        table_name="reflection_event_logs",
    )
    op.drop_index(
        "ix_reflection_event_logs_reflection_id", table_name="reflection_event_logs"
    )
    op.drop_index(
        "ix_reflection_event_logs_timestamp", table_name="reflection_event_logs"
    )
    op.drop_table("reflection_event_logs")
    op.drop_index("ix_task_event_logs_id", table_name="task_event_logs")
    op.drop_index("ix_task_event_logs_linked_hta_node_id", table_name="task_event_logs")
    op.drop_index("ix_task_event_logs_task_id", table_name="task_event_logs")
    op.drop_index("ix_task_event_logs_timestamp", table_name="task_event_logs")
    op.drop_table("task_event_logs")
    op.alter_column(
        "memory_snapshots",
        "snapshot_data",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        type_=postgresql.JSONB(astext_type=sa.Text()),
        nullable=True,
    )
    # --- Note: Handling potential NULLs before altering memory_snapshots.created_at ---
    # If created_at could also have NULLs, you'd add a similar op.execute() here:
    # op.execute("UPDATE memory_snapshots SET created_at = NOW() WHERE created_at IS NULL")
    op.alter_column(
        "memory_snapshots",
        "created_at",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        nullable=False,
        server_default=sa.text("now()"),
    )  # Added server_default for consistency
    op.drop_index("ix_memory_snapshots_codename", table_name="memory_snapshots")
    # Assuming the foreign key constraint name needs to be generated or is known
    # If None doesn't work, you might need to specify a name like 'fk_memory_snapshots_user_id_users'
    op.create_foreign_key(None, "memory_snapshots", "users", ["user_id"], ["id"])
    op.drop_column("memory_snapshots", "updated_at")

    # --- Note: Handling potential NULLs before altering users.is_active ---
    # If is_active could have NULLs and needs a default (e.g., True)
    # op.execute("UPDATE users SET is_active = TRUE WHERE is_active IS NULL")
    op.alter_column(
        "users",
        "is_active",
        existing_type=sa.BOOLEAN(),
        nullable=False,
        server_default=sa.true(),
    )  # Added server_default for consistency

    # --- Note: Handling potential NULLs before altering users.created_at ---
    # If created_at could also have NULLs, you'd add a similar op.execute() here:
    # op.execute("UPDATE users SET created_at = NOW() WHERE created_at IS NULL")
    op.alter_column(
        "users",
        "created_at",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        nullable=False,
        server_default=sa.text("now()"),
    )  # Added server_default for consistency

    # --- FIX FOR users.updated_at NULLS ---
    # Update existing rows where updated_at is NULL before applying NOT NULL constraint
    # Optional: for logging
    print("Updating NULL values in users.updated_at...")
    op.execute("UPDATE users SET updated_at = NOW() WHERE updated_at IS NULL")
    print("Finished updating NULL values.")  # Optional: for logging
    # --- END FIX ---

    # Now apply the NOT NULL constraint (and potentially type change)
    op.alter_column(
        "users",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(
            timezone=True
        ),  # Assuming this type is correct from models.py
        nullable=False,
        # Added server_default for consistency
        server_default=sa.text("now()"),
        existing_server_default=sa.text("now()"),
    )  # Added existing for onupdate

    op.drop_column("users", "full_name")
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Note: Reversing the update for NULLs isn't straightforward,
    # as we don't know which ones were originally NULL.
    # The downgrade path might need manual adjustment if this migration
    # needs to be reversible in practice.
    op.add_column(
        "users",
        sa.Column("full_name", sa.VARCHAR(), autoincrement=False, nullable=True),
    )
    op.alter_column(
        "users",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,  # Revert nullable back to True
        server_default=None,  # Remove server default if it was added
        existing_server_default=sa.text("now()"),
    )  # Keep existing if needed? Check model
    op.alter_column(
        "users",
        "created_at",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        nullable=True,  # Revert nullable back to True
        server_default=None,
    )  # Remove server default
    op.alter_column(
        "users",
        "is_active",
        existing_type=sa.BOOLEAN(),
        nullable=True,  # Revert nullable back to True
        server_default=None,
    )  # Remove server default
    op.add_column(
        "memory_snapshots",
        sa.Column(
            "updated_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
    )
    # Need the actual constraint name to drop it reliably
    # Replace 'fk_memory_snapshots_user_id_users' with the correct name if needed
    op.drop_constraint(
        "fk_memory_snapshots_user_id_users", "memory_snapshots", type_="foreignkey"
    )
    op.create_index(
        "ix_memory_snapshots_codename", "memory_snapshots", ["codename"], unique=False
    )
    op.alter_column(
        "memory_snapshots",
        "created_at",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        nullable=True,  # Revert nullable back to True
        server_default=None,
    )  # Remove server default
    op.alter_column(
        "memory_snapshots",
        "snapshot_data",
        existing_type=postgresql.JSONB(astext_type=sa.Text()),
        type_=postgresql.JSON(astext_type=sa.Text()),
        nullable=True,
    )  # Assuming it was nullable before? Check old model/DB state
    op.create_table(
        "task_event_logs",
        # ... (rest of the downgrade definition, ensure it matches the old state) ...
        # This part is complex and depends heavily on the exact previous state
        # It's often safer to restore from backup than to rely on complex downgrades
        # Placeholder for brevity:
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original task_event_logs definition
        sa.PrimaryKeyConstraint("id", name="task_event_logs_pkey"),
    )
    # Recreate indexes for task_event_logs
    op.create_table(
        "reflection_event_logs",
        # Placeholder for brevity:
        sa.Column("id", sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original reflection_event_logs definition
        sa.PrimaryKeyConstraint("id", name="reflection_event_logs_pkey"),
    )
    # Recreate indexes for reflection_event_logs
    op.drop_index(op.f("ix_task_footprints_user_id"), table_name="task_footprints")
    op.drop_index(op.f("ix_task_footprints_task_id"), table_name="task_footprints")
    op.drop_index(op.f("ix_task_footprints_id"), table_name="task_footprints")
    op.drop_table("task_footprints")
    op.drop_index(op.f("ix_reflection_logs_user_id"), table_name="reflection_logs")
    op.drop_index(op.f("ix_reflection_logs_id"), table_name="reflection_logs")
    op.drop_table("reflection_logs")
    # ### end Alembic commands ###
